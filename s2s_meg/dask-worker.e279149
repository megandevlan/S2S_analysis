2025-10-28 08:23:07,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://128.117.208.175:38177'
2025-10-28 08:23:08,197 - distributed.worker - INFO -       Start worker at: tcp://128.117.208.175:40591
2025-10-28 08:23:08,197 - distributed.worker - INFO -          Listening to: tcp://128.117.208.175:40591
2025-10-28 08:23:08,197 - distributed.worker - INFO -           Worker name:               PBSCluster-1
2025-10-28 08:23:08,197 - distributed.worker - INFO -          dashboard at:      128.117.208.175:46659
2025-10-28 08:23:08,197 - distributed.worker - INFO - Waiting to connect to: tcp://128.117.208.175:44489
2025-10-28 08:23:08,197 - distributed.worker - INFO - -------------------------------------------------
2025-10-28 08:23:08,197 - distributed.worker - INFO -               Threads:                          1
2025-10-28 08:23:08,197 - distributed.worker - INFO -                Memory:                   3.73 GiB
2025-10-28 08:23:08,197 - distributed.worker - INFO -       Local Directory: /glade/derecho/scratch/mdfowler/tmp/dask-scratch-space/worker-s7i771xi
2025-10-28 08:23:08,197 - distributed.worker - INFO - -------------------------------------------------
2025-10-28 08:23:08,423 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-10-28 08:23:08,424 - distributed.worker - INFO -         Registered to: tcp://128.117.208.175:44489
2025-10-28 08:23:08,424 - distributed.worker - INFO - -------------------------------------------------
2025-10-28 08:23:08,424 - distributed.core - INFO - Starting established connection to tcp://128.117.208.175:44489
2025-10-28 08:48:57,972 - distributed.core - INFO - Connection to tcp://128.117.208.175:44489 has been closed.
2025-10-28 08:48:57,972 - distributed.worker - INFO - Stopping worker at tcp://128.117.208.175:40591. Reason: worker-handle-scheduler-connection-broken
2025-10-28 08:48:57,978 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://128.117.208.175:38177'. Reason: worker-handle-scheduler-connection-broken
2025-10-28 08:48:57,979 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/utils.py", line 1949, in wait_for
    return await fut
           ^^^^^^^^^
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/comm/tcp.py", line 559, in connect
    convert_stream_closed_error(self, e)
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/comm/tcp.py", line 140, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x153d9cca0990>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/core.py", line 1623, in connect
    return await self._connect(addr=addr, timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/core.py", line 1567, in _connect
    comm = await connect(
           ^^^^^^^^^^^^^^
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/asyncio/tasks.py", line 649, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/core.py", line 1614, in connect
    async with asyncio.timeout(math.inf) as scope:
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/asyncio/timeouts.py", line 115, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/worker.py", line 1250, in heartbeat
    response = await retry_operation(
               ^^^^^^^^^^^^^^^^^^^^^^
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/utils_comm.py", line 459, in retry_operation
    return await retry(
           ^^^^^^^^^^^^
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/utils_comm.py", line 438, in retry
    return await coro()
           ^^^^^^^^^^^^
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/core.py", line 1635, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2025-10-28 08:48:57,994 - distributed.nanny - INFO - Worker closed
2025-10-28 08:49:00,470 - distributed.nanny - INFO - Closing Nanny at 'tcp://128.117.208.175:38177'. Reason: nanny-close-gracefully
2025-10-28 08:49:00,471 - distributed.dask_worker - INFO - End worker
