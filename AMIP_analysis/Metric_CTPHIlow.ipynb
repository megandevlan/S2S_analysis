{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac82345-cf48-467b-8484-5f20194aa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xarray.core.dataarray import DataArray\n",
    "import metpy.constants as mpconsts\n",
    "import numpy as np\n",
    "from metpy import calc as mpcalc\n",
    "from metpy.interpolate import log_interpolate_1d as log_interp_1d\n",
    "from metpy.units import units\n",
    "import os\n",
    "from pandas.core.series import Series\n",
    "from xarray.core.dataarray import DataArray\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np \n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import dask\n",
    "import metpy as mp\n",
    "from metpy.units import units\n",
    "import metpy.calc as mpc\n",
    "import Ngl\n",
    "\n",
    "# Plotting utils \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2cb3f-d43b-4f07-9ff5-375fa20bac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbed from Brian M. to use time midpoints, not end periods\n",
    "def cesm_correct_time(ds):\n",
    "    \"\"\"Given a Dataset, check for time_bnds,\n",
    "       and use avg(time_bnds) to replace the time coordinate.\n",
    "       Purpose is to center the timestamp on the averaging inverval.   \n",
    "       NOTE: ds should have been loaded using `decode_times=False`\n",
    "    \"\"\"\n",
    "    assert 'time_bnds' in ds\n",
    "    assert 'time' in ds\n",
    "    correct_time_values = ds['time_bnds'].mean(dim='nbnd')\n",
    "    # copy any metadata:\n",
    "    correct_time_values.attrs = ds['time'].attrs\n",
    "    ds = ds.assign_coords({\"time\": correct_time_values})\n",
    "    ds = xr.decode_cf(ds)  # decode to datetime objects\n",
    "    return ds\n",
    "\n",
    "## Function to regrid data from ADF\n",
    "def regrid_data(fromthis, tothis, method=1):\n",
    "    \"\"\"Regrid data using various different methods\"\"\"\n",
    "\n",
    "    #Import necessary modules:\n",
    "    import xarray as xr\n",
    "\n",
    "    if method == 1:\n",
    "        # kludgy: spatial regridding only, seems like can't automatically deal with time\n",
    "        if 'time' in fromthis.coords:\n",
    "            result = [fromthis.isel(time=t).interp_like(tothis) for t,time in enumerate(fromthis['time'])]\n",
    "            result = xr.concat(result, 'time')\n",
    "            return result\n",
    "        else:\n",
    "            return fromthis.interp_like(tothis)\n",
    "    elif method == 2:\n",
    "        newlat = tothis['lat']\n",
    "        newlon = tothis['lon']\n",
    "        coords = dict(fromthis.coords)\n",
    "        coords['lat'] = newlat\n",
    "        coords['lon'] = newlon\n",
    "        return fromthis.interp(coords)\n",
    "    elif method == 3:\n",
    "        newlat = tothis['lat']\n",
    "        newlon = tothis['lon']\n",
    "        ds_out = xr.Dataset({'lat': newlat, 'lon': newlon})\n",
    "        regridder = xe.Regridder(fromthis, ds_out, 'bilinear')\n",
    "        return regridder(fromthis)\n",
    "    elif method==4:\n",
    "        # geocat\n",
    "        newlat = tothis['lat']\n",
    "        newlon = tothis['lon']\n",
    "        result = geocat.comp.linint2(fromthis, newlon, newlat, False)\n",
    "        result.name = fromthis.name\n",
    "        \n",
    "        return result\n",
    "\n",
    "# - - - - - - - - - - - - - - - \n",
    "# Pre-process data while reading in \n",
    "# - - - - - - - - - - - - - - - \n",
    "\n",
    "def preprocess_atm(ds):\n",
    "    ds         = cesm_correct_time(ds)\n",
    "    dsSel      = ds[['SHFLX','LHFLX','PRECT']]\n",
    "    # Also select spatial range\n",
    "    dsSel = dsSel.sel(lat=slice(10,50), lon=slice(190,310))\n",
    "    return dsSel\n",
    "\n",
    "def preprocess_atm_profiles(ds):\n",
    "    ds         = cesm_correct_time(ds)\n",
    "    dsSel      = ds[['T','Q','PS','hyam','hyai','hybm','hybi','P0']]\n",
    "    # Also select spatial range\n",
    "    dsSel = dsSel.sel(lat=slice(10,50), lon=slice(190,310), lev=slice(500,1000))\n",
    "    return dsSel\n",
    "    \n",
    "def interpolateToPressure(DS, varName, pressGoals):\n",
    "    p0mb = DS.P0.values/100        # mb\n",
    "\n",
    "    # Pull out hya/hyb profiles \n",
    "    hyam = np.squeeze(DS.hyam.values)[:]\n",
    "    hybm = np.squeeze(DS.hybm.values)[:]\n",
    "    hyai = np.squeeze(DS.hyai.values)[:]\n",
    "    hybi = np.squeeze(DS.hybi.values)[:]\n",
    "\n",
    "    # Surface pressure with time dimension\n",
    "    PS   = DS.PS.values              # Pa \n",
    "\n",
    "    # Converting variables: \n",
    "    varInterp = Ngl.vinth2p(DS[varName].values,hyam,hybm,pressGoals,PS,1,p0mb,1,True)\n",
    "    \n",
    "    return varInterp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "447d3c0b-e445-42ee-bce3-88c19098612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33395 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://128.117.208.85:38043' processes=0 threads=0, memory=0 B>\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "dask.config.set({'logging.distributed': 'error'})\n",
    "\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "# For Casper\n",
    "cluster = PBSCluster(\n",
    "    queue=\"casper\",\n",
    "    walltime=\"02:00:00\",\n",
    "    account=\"P93300042\",\n",
    "    memory=\"4GB\",\n",
    "    resource_spec=\"select=1:ncpus=2:mem=4GB\",\n",
    "    cores=1,\n",
    "    processes=1,\n",
    ")\n",
    "\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Connect client to the remote dask workers\n",
    "client = Client(cluster)\n",
    "print(client)\n",
    "\n",
    "cluster.scale(8)\n",
    "\n",
    "client.wait_for_workers(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f22da-41ff-4bfc-9d2a-1d1db8fa2d28",
   "metadata": {},
   "source": [
    "## Get CESM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cde38e1-8d2a-428a-abb1-ea7618d7eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseNames = [\n",
    "            'f.e21.F2000climo.f09_f09_mg17.S2S_LandAtmCoupling_output.002',\n",
    "            'f.e21.F2000climo.f09_f09_mg17.ReduceDSL_0p8to0p5.S2S_LandAtmCoupling_output.002',\n",
    "            # 'i.e21.I2000Clm50Sp.f09_f09_mg17.S2S_LandAtmCoupling_output.002',\n",
    "            # 'f.e21.FHIST.f09_f09_mg17.S2S_LandAtmCoupling_output.002',\n",
    "           ]\n",
    "\n",
    "case_IDs = [\n",
    "            'F2000climo_ctrl',\n",
    "            'F2000climo_dsl0p5',\n",
    "            # 'I2000Clm50Sp',\n",
    "            # 'FHIST_ctrl', \n",
    "            ]\n",
    "\n",
    "dataDir = '/glade/campaign/cgd/tss/people/mdfowler/LandAtmCoupling_longRuns/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59729735-4bde-4a27-b469-706c7caadb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Starting on case F2000climo_ctrl ***\n",
      "atm 3D files loaded\n",
      "CPU times: user 28.9 s, sys: 5.96 s, total: 34.8 s\n",
      "Wall time: 5min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "iCase=0\n",
    "\n",
    "# for iCase in range(len(case_IDs)):\n",
    "print('*** Starting on case %s ***' % (case_IDs[iCase]))\n",
    "\n",
    "## Select files with daily means (h1 files in this case)\n",
    "listFiles_atm_lev = np.sort(glob.glob(dataDir+caseNames[iCase]+'/atm/hist/*cam.h3.????-*'))\n",
    "\n",
    "\n",
    "DS_case0 = xr.open_mfdataset(listFiles_atm_lev,  preprocess=preprocess_atm_profiles, concat_dim='time', \n",
    "                            combine='nested', decode_times=False, \n",
    "                            data_vars='minimal', parallel=True)\n",
    "\n",
    "print('atm 3D files loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "049c1877-bab1-420d-819d-c106af228888",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get landfrac\n",
    "h0_files = np.sort(glob.glob(dataDir+caseNames[iCase]+'/atm/hist/*cam.h0.????-*'))\n",
    "landfrac = xr.open_dataset(h0_files[1]).LANDFRAC\n",
    "landfrac = landfrac.isel(time=0).sel(lat=slice(10,50), lon=slice(190,310))\n",
    "\n",
    "landMask = np.ones([len(landfrac.lat.values), len(landfrac.lon.values)])\n",
    "landMask[np.squeeze(landfrac.values)<=0.45] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1383ba2c-2ad6-47b2-92c7-3c73e621e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop first year and only keep JJA \n",
    "iTimes = np.where( (DS_case0['time.year'].values>=(DS_case0['time.year'].values[0]+1))  & \n",
    "                   (DS_case0['time.month'].values>=6) & (DS_case0['time.month'].values<=8) \n",
    "                  )[0]\n",
    "\n",
    "DS_case0 = DS_case0.isel(time=iTimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a946aec-606e-475e-8942-c768dfbb9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "selHr = 13\n",
    "\n",
    "iHours = np.where( (DS_case0['time.hour'].values==13) )[0]\n",
    "len(iHours)\n",
    "\n",
    "DS_case0 = DS_case0.isel(time=iHours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02919c87-1bd3-4bed-84d5-3b64d7485609",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_interp = np.append(np.arange(550, 900, 50), np.arange(900,1000, 25))\n",
    "# P_interp = np.append(P_interp, 990)\n",
    "\n",
    "T_interp_temp = interpolateToPressure(DS_case0, 'T', P_interp)\n",
    "\n",
    "T_interp = xr.DataArray(T_interp_temp, \n",
    "    coords={\n",
    "            'time': DS_case0.time.values,\n",
    "            'lev':  P_interp,\n",
    "            'lat':  DS_case0.lat.values, \n",
    "            'lon':  DS_case0.lon.values}, \n",
    "    dims=[\"time\", \"lev\", \"lat\", \"lon\"])\n",
    "\n",
    "T_invertLev = T_interp.isel(lev=slice(None, None, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c659686e-c047-4597-85cb-c2498da99927",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_interp = np.append(np.arange(550, 900, 50), np.arange(900,1000, 25))\n",
    "# P_interp = np.append(P_interp, 990)\n",
    "\n",
    "Q_interp_temp = interpolateToPressure(DS_case0, 'Q', P_interp)\n",
    "\n",
    "Q_interp = xr.DataArray(Q_interp_temp, \n",
    "    coords={\n",
    "            'time': DS_case0.time.values,\n",
    "            'lev':  P_interp,\n",
    "            'lat':  DS_case0.lat.values, \n",
    "            'lon':  DS_case0.lon.values}, \n",
    "    dims=[\"time\", \"lev\", \"lat\", \"lon\"])\n",
    "\n",
    "Q_invertLev = Q_interp.isel(lev=slice(None, None, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "360a2d32-618a-4e37-abaf-04e28ecfcbbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdir = '/glade/derecho/scratch/mdfowler/S2S_processed/'\n",
    "\n",
    "ctp_10utc = xr.open_dataset(outdir+caseNames[0]+'_CTP_JJA_10utc.nc')\n",
    "ctp_13utc = xr.open_dataset(outdir+caseNames[0]+'_CTP_JJA_13utc.nc')\n",
    "\n",
    "\n",
    "## Around longitude = 110, switch from 13 UTC to 10 UTC. Could also try 105, which is ~longitude of Denver (going with Denver) \n",
    "iLon = np.where( (T_invertLev.lon.values>=254) & (T_invertLev.lon.values<=256))[0] \n",
    "\n",
    "ctp_combined = ctp_13utc.__xarray_dataarray_variable__.values\n",
    "ctp_combined[:,:,iLon[0]::] = ctp_10utc.__xarray_dataarray_variable__.values[:,:,iLon[0]::]\n",
    "\n",
    "ctp = xr.DataArray(ctp_combined, \n",
    "    coords={\n",
    "            'time': T_invertLev.time.values,\n",
    "            'lat':  T_invertLev.lat.values, \n",
    "            'lon':  T_invertLev.lon.values}, \n",
    "    dims=[\"time\", \"lat\", \"lon\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61305e22-adba-4c75-971b-b8a05218987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctp_noZeros = ctp.where(ctp.values>=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c35dabe-59aa-4f98-a2ef-dd0dd88261bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/apps/opt/conda/envs/npl-2024b/lib/python3.11/site-packages/metpy/calc/thermo.py:1396: RuntimeWarning: invalid value encountered in log\n",
      "  val = np.log(vapor_pressure / mpconsts.nounit.sat_pressure_0c)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2300,42,97) (18400,42,97) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m tmpBot \u001b[38;5;241m=\u001b[39m  T_invertLev\u001b[38;5;241m.\u001b[39msel(lev\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m950\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m273.15\u001b[39m\n\u001b[1;32m     10\u001b[0m tmpTop \u001b[38;5;241m=\u001b[39m  T_invertLev\u001b[38;5;241m.\u001b[39msel(lev\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m850\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m273.15\u001b[39m\n\u001b[0;32m---> 12\u001b[0m HIlow \u001b[38;5;241m=\u001b[39m (\u001b[43mtmpBot\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mdewpoint_950\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m) \u001b[38;5;241m+\u001b[39m (tmpTop\u001b[38;5;241m-\u001b[39mdewpoint_850\u001b[38;5;241m.\u001b[39mm)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2300,42,97) (18400,42,97) "
     ]
    }
   ],
   "source": [
    "## Also get humidity index piece \n",
    "\n",
    "dewpoint_950 = mpc.dewpoint_from_specific_humidity(Q_invertLev.sel(lev=950, method='nearest').lev.values * units.hPa, \n",
    "                                                   Q_invertLev.sel(lev=950, method='nearest').values * units(DS_case0.Q.units))\n",
    "\n",
    "dewpoint_850 = mpc.dewpoint_from_specific_humidity(Q_invertLev.sel(lev=850, method='nearest').lev.values * units.hPa, \n",
    "                                                   Q_invertLev.sel(lev=850, method='nearest').values * units(DS_case0.Q.units))\n",
    "\n",
    "tmpBot =  T_invertLev.sel(lev=950, method='nearest').values -273.15\n",
    "tmpTop =  T_invertLev.sel(lev=850, method='nearest').values -273.15\n",
    "\n",
    "HIlow = (tmpBot-dewpoint_950.m) + (tmpTop-dewpoint_850.m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac1be3-118e-4dc8-9b7a-4e6de6803efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "HI_low = xr.DataArray(HIlow, \n",
    "    coords={\n",
    "            'time': T_invertLev.time.values,\n",
    "            'lat':  T_invertLev.lat.values, \n",
    "            'lon':  T_invertLev.lon.values}, \n",
    "    dims=[\"time\", \"lat\", \"lon\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37345ed-c38d-473e-ab90-07f4a7cfe648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "160d5d47-6047-4772-b6ed-2b02f651bf3c",
   "metadata": {},
   "source": [
    "## Get ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54036d62-f75f-437c-bd1a-6f5f3a018ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_era5(DS):\n",
    "    ## Select only hours 12-15 UTC\n",
    "    # iHours = np.where( (DS['time.hour']>=12) & (DS['time.hour']<=14))[0]\n",
    "    ## Select only hours 9-11 UTC\n",
    "    iHours = np.where( (DS['time.hour']>=9) & (DS['time.hour']<=11))[0]\n",
    "    iLevs  = np.where(DS.level.values>=500)[0]\n",
    "    DS_sel = DS.isel(time=iHours, level=iLevs).resample(time='1D').mean()\n",
    "\n",
    "    \n",
    "    ## Regrid\n",
    "    DS_sel  = DS_sel.rename({'longitude': 'lon','latitude': 'lat'})\n",
    "    camGrid = DS_cam['PS'].isel(time=0).load().squeeze()\n",
    "    regridERA = regrid_data(DS_sel, camGrid, method=1)\n",
    "        \n",
    "    DS_sel2 = regridERA.sel(lat=slice(15,55), lon=slice(225, 300))\n",
    "\n",
    "        \n",
    "    return DS_sel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ddb085-6ef5-46e4-83c0-67e5c4d6a59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '/glade/campaign/collections/rda/data/d633000/e5.oper.an.pl/'\n",
    "\n",
    "years = 1995+np.arange(11)\n",
    "print(years)\n",
    "\n",
    "months = ['06','07','08']\n",
    "\n",
    "count = 0 \n",
    "for iYr in range(len(years)):\n",
    "    for iMon in range(len(months)):\n",
    "        listFiles_q_temp = np.sort(glob.glob(dataDir+str(years[iYr])+months[iMon]+'/*_q*.nc'))\n",
    "        listFiles_t_temp = np.sort(glob.glob(dataDir+str(years[iYr])+months[iMon]+'/*_t*.nc'))\n",
    "\n",
    "        if count==0:\n",
    "            listFiles_q = listFiles_q_temp\n",
    "            listFiles_t = listFiles_t_temp\n",
    "        else: \n",
    "            listFiles_q = np.append(listFiles_q, listFiles_q_temp)\n",
    "            listFiles_t = np.append(listFiles_t, listFiles_t_temp)\n",
    "\n",
    "        count = count+1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62375867-e319-4ccc-adf5-78c92ec2f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "q_era5 = xr.open_mfdataset(np.sort(listFiles_q), preprocess=preprocess_era5, \n",
    "                           # combine='by_coords',\n",
    "                           combine='nested', concat_dim='time', \n",
    "                           decode_times=True, data_vars='minimal', parallel=True, \n",
    "                           # chunks={'time': 150},\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a40a1c-fb5f-404a-ad5e-af910bfa49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_era5 = xr.open_mfdataset(np.sort(listFiles_t), preprocess=preprocess_era5, \n",
    "                                combine='by_coords',\n",
    "                               #concat_dim='time', combine='nested', \n",
    "                                 decode_times=True, data_vars='minimal', parallel=True,\n",
    "                                 chunks={'time': 150},\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb677eea-2ad7-4459-8e8a-2c4ab9529055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2373958-783f-4ac9-b6df-3b09e13becce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2024b",
   "language": "python",
   "name": "npl-2024b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
